STATISTICS IN PYTHON
===========CHAPTER ONE: Summary Statistics
Descriptive and inferential statistics

Statistics can be used to answer lots of different types of questions, but 
being able to identify which type of statistics is needed is essential to 
drawing accurate conclusions. In this exercise, you'll sharpen your skills 
by identifying which type is needed to answer each question.

Instructions

Identify which questions can be answered with descriptive statistics and 
which questions can be answered with inferential statistics.

Descriptive
---Given data on every customer service request made, what's the average
time it took to respond?

---Given data on all 100,000 people who viewed an ad, what percent of 
people clicked on it?

Inferential
---After interviewing 100 customers, what percent of all your customers are 
satisfied with your product?

---Given data on 20 fish caught in a lake, what's the average weight of 
all fish in the lake?


Nice work! Knowing the type of statistics you need to answer your question 
will help you choose the appropriate methods to get the most accurate 
answer possible.

================Data type classification
In the video, you learned about two main types of data: numeric and 
categorical. Numeric variables can be classified as either discrete or 
continuous, and categorical variables can be classified as either nominal 
or ordinal. These characteristics of a variable determine which ways of 
summarizing your data will work best.

Instructions

Map each variable to its data type by dragging each item and dropping it 
into the correct data type.

Continuous numberic
--Air temperature; Kilowatts of electricity used

Discrete numeric
--Number of itmes in stock; Number of DataCamp courses taken,
Number of clicks on an ad

Categorical
--Zip code; Brand of a product.


Superb sorting skills! These skills will be important when it comes to 
choosing summary statistics and visualizations.

===============Mean and median
In this chapter, you'll be working with the 2018 Food Carbon Footprint 
Index from nu3. The food_consumption dataset contains information about 
the kilograms of food consumed per person per year in each country in 
each food category (consumption) as well as information about the carbon 
footprint of that food category (co2_emissions) measured in kilograms of 
carbon dioxide, or CO2, per person per year in each country.

In this exercise, you'll compute measures of center to compare food 
consumption in the US and Belgium using your pandas and numpy skills.

pandas is imported as pd for you and food_consumption is pre-loaded.

Instructions

Import numpy with the alias np.

Create two DataFrames: one that holds the rows of food_consumption for 
'Belgium' and another that holds rows for 'USA'. Call these 
be_consumption and usa_consumption.

-Calculate the mean and median of kilograms of food consumed per person 
per year for both countries.

# Import numpy with alias np
import numpy as np

# Filter for Belgium
be_consumption = food_consumption[food_consumption['country'] == 'Belgium']

# Filter for USA
usa_consumption = food_consumption[food_consumption['country'] == 'USA']

# Calculate mean and median consumption in Belgium
print(np.mean(be_consumption['consumption']))
print(np.median(be_consumption['consumption']))

# Calculate mean and median consumption in USA
print(np.mean(usa_consumption['consumption']))
print(np.median(usa_consumption['consumption']))


Subset food_consumption for rows with data about Belgium and the USA.

Group the subsetted data by country and select only the consumption column.

-Calculate the mean and median of the kilograms of food consumed per 
person per year in each country using .agg().

# Import numpy as np
import numpy as np

# Subset for Belgium and USA only
be_and_usa = food_consumption[(food_consumption['country'] == 'Belgium') | 
(food_consumption['country'] == 'USA')]

# Group by country, select consumption column, and compute mean and median
print(be_and_usa.groupby('country')['consumption'].agg([np.mean, np.median]))

Marvelous mean and median calculation! When you want to compare summary 
statistics between groups, it's much easier to use .groupby() and .agg() 
instead of subsetting and calling the same functions multiple times.

===============Mean vs. median
In the video, you learned that the mean is the sum of all the data points 
divided by the total number of data points, and the median is the middle 
value of the dataset where 50% of the data is less than the median, and 50% 
of the data is greater than the median. In this exercise, you'll compare 
these two measures of center.

pandas is loaded as pd, numpy is loaded as np, and food_consumption is 
available.

Instructions

Import matplotlib.pyplot with the alias plt.
Subset food_consumption to get the rows where food_category is 'rice'.
Create a histogram of co2_emission for rice and show the plot.

# Import matplotlib.pyplot with alias plt
import matplotlib.pyplot as plt

# Subset for food_category equals rice
rice_consumption = food_consumption[food_consumption['food_category'] == 'rice']

# Histogram of co2_emission for rice and show plot
rice_consumption['co2_emission'].hist()
plt.show()

Question
Take a look at the histogram you just created of different countries' CO2 
emissions for rice. Which of the following terms best describes the shape of 
the data?

Answer
--- Right-skewed

Use .agg() to calculate the mean and median of co2_emission for rice.

# Subset for food_category equals rice
rice_consumption = food_consumption[food_consumption['food_category'] == 'rice']

# Calculate mean and median of co2_emission with .agg()
print(rice_consumption.agg([np.mean, np.median]))


Question
Given the skew of this data, what measure of central tendency best summarizes 
the kilograms of CO2 emissions per person per year for rice?

Answer: Median

Great work! The mean is substantially higher than the median since 
it's being pulled up by the high values over 100 kg/person/year.

===============Quartiles, quantiles, and quintiles
Quantiles are a great way of summarizing numerical data since they can be used 
to measure center and spread, as well as to get a sense of where a data point 
stands in relation to the rest of the data set. For example, you might want to 
give a discount to the 10% most active users on a website.

In this exercise, you'll calculate quartiles, quintiles, and deciles, which 
split up a dataset into 4, 5, and 10 pieces, respectively.

Both pandas as pd and numpy as np are loaded and food_consumption is available.

--Calculate the quartiles of the co2_emission column of food_consumption.

# Calculate the quartiles of co2_emission
print(np.quantile(food_consumption['co2_emission'], np.linspace(0,1,5)))

Calculate the six quantiles that split up the data into 5 pieces (quintiles) 
of the co2_emission column of food_consumption.

# Calculate the quintiles of co2_emission

print(np.quantile(food_consumption['co2_emission'], np.linspace(0,1,6)))

Calculate the eleven quantiles of co2_emission that split up the data into 
ten pieces (deciles).

# Calculate the deciles of co2_emission

print(np.quantile(food_consumption['co2_emission'], np.linspace(0,1,11)))


Those are some high-quality quantiles! While calculating more quantiles gives 
you a more detailed look at the data, it also produces more numbers, making 
the summary more difficult to quickly understand.

===============Variance and standard deviation
Variance and standard deviation are two of the most common ways to measure 
the spread of a variable, and you'll practice calculating these in this 
exercise. Spread is important since it can help inform expectations. For 
example, if a salesperson sells a mean of 20 products a day, but has a 
standard deviation of 10 products, there will probably be days where they 
sell 40 products, but also days where they only sell one or two. Information 
like this is important, especially when making predictions.

Both pandas as pd and numpy as np are loaded, and food_consumption is available.

Instructions

Calculate the variance and standard deviation of co2_emission for each 
food_category by grouping and aggregating.

Import matplotlib.pyplot with alias plt.

Create a histogram of co2_emission for the beef food_category and show the plot.

Create a histogram of co2_emission for the eggs food_category and show the plot.

# Print variance and sd of co2_emission for each food_category
print(food_consumption.groupby('food_category')['co2_emission'].agg([np.var, np.std]))

# Import matplotlib.pyplot with alias plt
import matplotlib.pyplot as plt

# Create histogram of co2_emission for food_category 'beef'
food_consumption[food_consumption['food_category'] == 'beef']['co2_emission'].hist()

# Show plot
plt.show()

# Create histogram of co2_emission for food_category 'eggs'
food_consumption[food_consumption['food_category'] == 'eggs']['co2_emission'].hist()

# Show plot
plt.show()


Superb spread measurement! Beef has the largest amount of variation in its 
CO2 emissions, while eggs have a relatively small amount of variation.

===============Finding outliers using IQR
Outliers can have big effects on statistics like mean, as well as statistics 
that rely on the mean, such as variance and standard deviation. Interquartile 
range, or IQR, is another way of measuring spread that's less influenced by 
outliers. IQR is also often used to find outliers. If a value is less than
Q1 - 1.5 * IQR or greater than Q3 + 1.5 * IQR, it's considered an outlier. In 
fact, this is how the lengths of the whiskers in a matplotlib box plot are 
calculated.

[ https://assets.datacamp.com/production/repositories/5758/datasets/
ca7e6e1832be7ec1842f62891815a9b0488efa83/Screen%20Shot%
202020-04-28%20at%2010.04.54%20AM.png ]

In this exercise, you'll calculate IQR and use it to find some outliers. 
pandas as pd and numpy as np are loaded and food_consumption is available.

--Calculate the total co2_emission per country by grouping by country and 
taking the sum of co2_emission. Store the resulting DataFrame as 
emissions_by_country.

# Calculate total co2_emission per country: emissions_by_country
emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()

print(emissions_by_country)

--Compute the first and third quartiles of emissions_by_country and store these 
as q1 and q3.

--Calculate the interquartile range of emissions_by_country and store it as iqr.

# Calculate total co2_emission per country: emissions_by_country
emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()

# Compute the first and third quartiles and IQR of emissions_by_country
q1 = np.quantile(emissions_by_country, 0.25) 
q3 = np.quantile(emissions_by_country, 0.75)
iqr = q3 - q1

Calculate the lower and upper cutoffs for outliers of emissions_by_country, and 
store these as lower and upper.

# Calculate total co2_emission per country: emissions_by_country
emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()

# Compute the first and third quantiles and IQR of emissions_by_country
q1 = np.quantile(emissions_by_country, 0.25)
q3 = np.quantile(emissions_by_country, 0.75)
iqr = q3 - q1

# Calculate the lower and upper cutoffs for outliers
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr

Subset emissions_by_country to get countries with a total emission greater 
than the upper cutoff or a total emission less than the lower cutoff.

# Calculate total co2_emission per country: emissions_by_country
emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()

# Compute the first and third quantiles and IQR of emissions_by_country
q1 = np.quantile(emissions_by_country, 0.25)
q3 = np.quantile(emissions_by_country, 0.75)
iqr = q3 - q1

# Calculate the lower and upper cutoffs for outliers
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr

# Subset emissions_by_country to find outliers
outliers = emissions_by_country[(emissions_by_country > upper) | (emissions_by_country < lower)]
print(outliers)

Outstanding outlier detection! It looks like Argentina has a substantially 
higher amount of CO2 emissions per person than other countries in the world.

===============

